---
title: winç³»ç»Ÿä¸‹DeepSeek-OCR2éƒ¨ç½²ä¸ä½¿ç”¨
published: 2026-01-30
tags: [DeepSeek-OCR2]
category: å¼€æºéƒ¨ç½²
image: "./9.webp"
draft: false
licenseName: "CC BY-NC-SA 4.0"
---
# æ¨¡å‹ä¸‹è½½
## è®¾ç½®é­”å¡”ç¤¾åŒºï¼ˆModelScopeï¼‰ä¸‹è½½è·¯å¾„
é…ç½®ç¯å¢ƒå˜é‡ï¼Œå°†é­”å¡”ç¤¾åŒºä¿å­˜æ¨¡å‹çš„è·¯å¾„ä»é»˜è®¤çš„Cç›˜è°ƒæ•´è‡³Dç›˜ã€‚æŒ‰ç…§å¦‚ä¸‹æ­¥éª¤è¿›è¡Œï¼š
- åœ¨ Windows æœç´¢æ è¾“å…¥â€œç¼–è¾‘ç³»ç»Ÿç¯å¢ƒå˜é‡â€ï¼Œç‚¹å‡»â€œç¯å¢ƒå˜é‡â€æŒ‰é’®ã€‚
- åœ¨â€œç”¨æˆ·å˜é‡â€æˆ–â€œç³»ç»Ÿå˜é‡â€åŒºåŸŸï¼Œç‚¹å‡»â€œæ–°å»ºâ€ã€‚
    - å˜é‡å: `MODELSCOPE_CACHE`
    - å˜é‡å€¼: `D:\ModelScope_Cache` (æˆ–è€…æ˜¯ä½ æƒ³è¦å­˜æ”¾æ¨¡å‹çš„ D ç›˜å…·ä½“è·¯å¾„)
    - ä¿å­˜å¹¶é‡å¯ä½ çš„ç»ˆç«¯ï¼ˆCMD/PowerShellï¼‰æˆ– æ‰€æœ‰çš„IDEï¼ˆVS Code/PyCharmï¼‰
- æ–°å»ºæ–‡ä»¶å¤¹ModelScope_Cacheä¸deepseek-OCR2ï¼Œåœ¨deepseek-OCR2æ–‡ä»¶å¤¹å†…æ‰“å¼€VS codeï¼ˆIDEï¼‰
## ä¸‹è½½æ¨¡å‹æƒé‡
åœ¨VS codeä¸­æ–°å»ºé»˜è®¤ç»ˆç«¯ï¼Œåˆ›å»ºæ–°ç¯å¢ƒï¼š
```base
conda create -n dsocr2 python=3.12.9 -y
conda activate dsocr2
```
æ¿€æ´»ç¯å¢ƒåï¼Œå®‰è£…é­”å¡”ç¤¾åŒºåŒ…å¹¶ä¸‹è½½æ¨¡å‹æƒé‡ï¼š
```dsocr2
pip install modelscope
modelscope download --model deepseek-ai/DeepSeek-OCR-2
```
## ä¸‹è½½æ¨¡å‹æºç 
ä½¿ç”¨gitå…‹éš†æºç å¹¶è¿›å…¥é¡¹ç›®æ–‡ä»¶å¤¹ï¼š
```dsocr2
git clone https://github.com/deepseek-ai/DeepSeek-OCR-2.git
cd DeepSeek-OCR-2
```
ä¾æ¬¡å®‰è£…å¿…å¤‡åŒ…ï¼š
```dsocr2
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 =2.6.0 --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt

```
:::note[æ¨¡å‹å­˜æ”¾Dç›˜çš„ç›¸å…³å¯¼å…¥æ³¨æ„äº‹é¡¹]
**1.æƒ…å†µä¸€ï¼šä½¿ç”¨ ModelScope å®˜æ–¹ SDK åŠ è½½ï¼ˆæœ€çœäº‹ï¼‰**

å¦‚æœä½ åœ¨ä»£ç ä¸­ä½¿ç”¨ `snapshot_download` æˆ– `AutoModel` æ¥åŠ è½½æ¨¡å‹ï¼Œåªè¦ä½ è®¾ç½®äº†ç¯å¢ƒå˜é‡ `MODELSCOPE_CACHE`ï¼Œä»£ç ä¼šè‡ªåŠ¨å»æ–°è·¯å¾„æ‰¾æ¨¡å‹ï¼Œä¸éœ€è¦æ”¹ä»£ç ã€‚
```python
import os
# å¦‚æœä½ æ²¡æœ‰åœ¨ç³»ç»Ÿå±‚é¢è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¯ä»¥åœ¨ä»£ç æœ€å¼€å¤´æ‰‹åŠ¨æŒ‡å®š
# os.environ['MODELSCOPE_CACHE'] = 'D:\\ModelScope_Cache' 

from modelscope import snapshot_download
# è¿™è¡Œä»£ç ä¼šè‡ªåŠ¨å» D:\ModelScope_Cache æŸ¥æ‰¾ï¼Œæ‰¾ä¸åˆ°æ‰ä¼šä¸‹è½½
model_dir = snapshot_download('deepseek-ai/DeepSeek-OCR-2') 
```
**2.æƒ…å†µäºŒï¼šæ‰‹åŠ¨æŒ‡å®šç»å¯¹è·¯å¾„**

å¦‚æœä½ ä¸æƒ³ä¾èµ–ç¯å¢ƒå˜é‡ï¼Œä½ éœ€è¦çŸ¥é“æ¨¡å‹ä¸‹è½½åçš„å…·ä½“æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¹¶åœ¨ä»£ç ä¸­æ˜¾å¼æŒ‡å®šï¼š
```python
# å‡è®¾ä½ ä¸‹è½½åˆ°äº† D:\ModelScope_Cache\hub\models\deepseek-ai\DeepSeek-OCR-2
model_path = r"D:\ModelScope_Cache\hub\models\deepseek-ai\DeepSeek-OCR-2"

# åŠ è½½æ—¶ç›´æ¥ä¼ è·¯å¾„
model = AutoModel.from_pretrained(model_path, ...)
```
æ³¨æ„äº‹é¡¹ï¼š

1.**è·¯å¾„åˆ†éš”ç¬¦**: åœ¨ Windows ä¸Šå†™è·¯å¾„å­—ç¬¦ä¸²æ—¶ï¼Œå»ºè®®åœ¨å­—ç¬¦ä¸²å‰åŠ  `r`ï¼ˆå¦‚ `r"D:\Models"`ï¼‰æˆ–è€…ä½¿ç”¨åŒåæ–œæ  `\\`ï¼Œé˜²æ­¢è½¬ä¹‰å­—ç¬¦æŠ¥é”™ã€‚

2.**HuggingFace å…¼å®¹æ€§**: å¦‚æœä½ ä¸æ˜¯ç”¨ modelscope çš„åº“ï¼Œè€Œæ˜¯ç”¨ `transformers` åº“åŠ è½½è¿™ä¸ªä¸‹è½½å¥½çš„æ¨¡å‹ï¼Œä½ éœ€è¦ç¡®ä¿ä¼ å…¥çš„æ˜¯åŒ…å« `config.json` å’Œ `.bin/.safetensors` æ–‡ä»¶çš„æœ€ç»ˆç›®å½•è·¯å¾„ã€‚

å¼ºçƒˆå»ºè®®è®¾ç½®æ°¸ä¹…ç¯å¢ƒå˜é‡ `MODELSCOPE_CACHE` åˆ° D ç›˜ï¼Œè¿™æ ·æ—¢èƒ½èŠ‚çœ C ç›˜ç©ºé—´ï¼Œåˆèƒ½åœ¨åç»­ä½¿ç”¨ä¸­ä¿æŒä»£ç ç®€æ´ï¼ˆä¸éœ€è¦æ¯æ¬¡éƒ½æ‰‹åŠ¨æ”¹è·¯å¾„ï¼‰ã€‚
:::
# æ¨¡å‹ä½¿ç”¨
## æ–‡ä»¶å¤¹åˆ›å»º

åœ¨æºç æ ¹ç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶å¤¹ï¼š`img_input`ã€`img_output`ã€`pdf_input`ã€`pdf_output`ï¼Œåˆ†åˆ«ç”¨äºå­˜æ”¾æ–‡ä»¶

## pdfè½¬md

winç³»ç»Ÿä¸‹ä½¿ç”¨deepseek-ocr2æˆ‘ä»¬æ— æ³•å®‰è£…`vllm`å’Œ`flash-attn`ï¼Œæ•…éœ€è¦é’ˆå¯¹**pdfè½¬md**è¿›è¡Œä¿®æ”¹ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ç±»ä¼¼å›¾ç‰‡è½¬mdçš„åŸºäº`transformers`çš„çº¯åŸç”Ÿæ–¹æ¡ˆ,è™½ç„¶æ¨ç†é€Ÿåº¦æ¯” vLLM ç¨æ…¢ï¼Œä½†èƒœåœ¨Windows å…¼å®¹æ€§å®Œç¾ï¼Œä¸”ä¸éœ€è¦å¤æ‚çš„ç¼–è¯‘ç¯å¢ƒã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨`DeepSeek-OCR2-hf`æ–‡ä»¶å¤¹ä¸­åˆ›å»º`run_dpsk_ocr2_pdf.py`æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š
```python
import os
import io
import fitz  # PyMuPDF
import torch
import sys
import contextlib
import re
from PIL import Image
from transformers import AutoModel, AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================
PDF_PATH = r' '#æ­¤å¤„å¡«å…¥pdf_inputä¸‹çš„pdfè·¯å¾„ï¼Œæ¨èä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œè·¯å¾„å†…å®¹å¿…é¡»ä»¥.pdfç»“å°¾,ä¾‹å¦‚ï¼šä¾‹å¦‚ï¼šDï¼š...\pdf_output\2500836.pdf
MODEL_PATH = r'D:\ModelScope_Cache\models\deepseek-ai\DeepSeek-OCR-2'#æ­¤å¤„å¡«å…¥ä½ çš„æ¨¡å‹æƒé‡æ‰€åœ¨è·¯å¾„
OUTPUT_MD_PATH = r' '#æ­¤å¤„å¡«å…¥pdf_outputä¸‹çš„mdè·¯å¾„ï¼Œæ¨èä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œè·¯å¾„å†…å®¹å¿…é¡»ä»¥.mdç»“å°¾ï¼Œå³åœ¨æ­¤éœ€å¡«å…¥ç›®å½•ä¸‹çš„mdæ–‡ä»¶ï¼Œä½†ä¸å¿…çœŸæœ‰æ­¤æ–‡ä»¶ï¼Œè½¬æ¢åä¼šè‡ªåŠ¨åˆ›å»ºï¼Œä½†æ˜¯æœ€åçš„è·¯å¾„ç»“å°¾ä¸èƒ½æ˜¯æ–‡ä»¶å¤¹ï¼ä¾‹å¦‚ï¼šDï¼š...\pdf_output\A2500836.md
# ===========================================

def clean_ocr_output(text):
    """
    æ¸…æ´—å™¨ï¼šä¸“é—¨å»é™¤æ¨¡å‹è¾“å‡ºä¸­çš„åƒåœ¾æ—¥å¿—
    """
    lines = text.split('\n')
    clean_lines = []
    
    # å®šä¹‰åƒåœ¾æ—¥å¿—çš„ç‰¹å¾å…³é”®è¯
    junk_keywords = [
        "The attention mask", "Setting `pad_token_id`", "The `seen_tokens` attribute",
        "BASE:  torch.Size", "PATCHES:  torch.Size", "UserWarning:", 
        "configuration_utils.py", "modeling_deepseekocr2.py", "warnings.warn",
        "Loading checkpoint", "input_ids", "attention_mask", "position_ids",
        "Using `bitsandbytes`", "was not set", "cannot be inferred",
        "Using the slow distutils backend", "get_max_cache", "transitioning from computing"
    ]

    for line in lines:
        is_junk = False
        # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«åƒåœ¾å…³é”®è¯
        for kw in junk_keywords:
            if kw in line:
                is_junk = True
                break
        
        # 2. æ£€æŸ¥æ˜¯å¦æ˜¯çº¯ç²¹çš„è¿›åº¦æ¡æˆ–åˆ†å‰²çº¿ï¼ˆå¦‚æœä¸æ˜¯Markdownçš„ä¸€éƒ¨åˆ†ï¼‰
        if line.strip().startswith("=====") and "torch.Size" not in line:
             # æœ‰äº›åˆ†å‰²çº¿å¯èƒ½æ˜¯æ—¥å¿—çš„ï¼Œæœ‰äº›æ˜¯æ–‡æ¡£çš„ï¼Œè¿™é‡Œä¿å®ˆå¤„ç†
             pass 

        if not is_junk:
            clean_lines.append(line)
            
    # é‡æ–°ç»„åˆæ–‡æœ¬
    result = '\n'.join(clean_lines)
    
    # 3. å»é™¤é¦–å°¾å¤šä½™ç©ºè¡Œ
    result = result.strip()
    return result

def load_pdf_as_images(pdf_path, dpi=144):
    print(f"[1/3] Loading PDF: {pdf_path}")
    images = []
    try:
        pdf_document = fitz.open(pdf_path)
        zoom = dpi / 72.0
        matrix = fitz.Matrix(zoom, zoom)
        for page_num in range(pdf_document.page_count):
            page = pdf_document[page_num]
            pixmap = page.get_pixmap(matrix=matrix, alpha=False)
            img = Image.open(io.BytesIO(pixmap.tobytes("png"))).convert("RGB")
            images.append(img)
        pdf_document.close()
        return images
    except Exception as e:
        print(f"Error: {e}")
        return []

def main():
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'
    
    print(f"[2/3] Loading Model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    try:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, _attn_implementation='flash_attention_2')
    except:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True)

    model = model.eval().cuda().to(torch.bfloat16)
    
    images = load_pdf_as_images(PDF_PATH)
    if not images: return

    full_markdown_content = ""
    print(f"[3/3] Start Recognition ({len(images)} pages)...")
    
    prompt = "<image>\nFree OCR."

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(OUTPUT_MD_PATH), exist_ok=True)

    for i, img in enumerate(images):
        print(f"Processing Page {i+1}/{len(images)}...", end="")
        
        temp_img_path = f"temp_page_{i}.png"
        img.save(temp_img_path)
        
        captured_output = io.StringIO()
        
        try:
            # æ•è·è¾“å‡º
            with contextlib.redirect_stdout(captured_output):
                # åŒæ—¶ä¹Ÿæ•è· stderr é˜²æ­¢è­¦å‘Šæ¼ç½‘
                with contextlib.redirect_stderr(captured_output):
                    model.infer(
                        tokenizer, 
                        prompt=prompt, 
                        image_file=temp_img_path, 
                        output_path="./temp_output", # è¿™ä¸ªè·¯å¾„ä¸é‡è¦
                        base_size=1024, 
                        image_size=768, 
                        crop_mode=True, 
                        save_results=False
                    )
            
            # è·å–åŸå§‹æ‚ä¹±æ–‡æœ¬
            raw_text = captured_output.getvalue()
            
            # === æ ¸å¿ƒæ­¥éª¤ï¼šæ¸…æ´—æ–‡æœ¬ ===
            clean_text = clean_ocr_output(raw_text)
            
            # æ‹¼æ¥åˆ°ç»“æœ
            if clean_text:
                full_markdown_content += f"\n\n<!-- Page {i+1} -->\n\n" + clean_text
                print(f" Done. (Len: {len(clean_text)})")
            else:
                print(f" Warning: No content.")

        except Exception as e:
            print(f" Error: {e}")
        finally:
            captured_output.close()
            if os.path.exists(temp_img_path):
                try: os.remove(temp_img_path) 
                except: pass

    # æœ€ç»ˆä¿å­˜
    with open(OUTPUT_MD_PATH, "w", encoding="utf-8") as f:
        f.write(full_markdown_content)
    
    print(f"\nSaved clean markdown to: {OUTPUT_MD_PATH}")

if __name__ == "__main__":
    main()
```
ä¿®æ”¹ä¸Šè¿°ä»£ç ä¸­çš„`PDF_PATH`ã€`MODEL_PATH`å’Œ`OUTPUT_MD_PATH`åå¹¶è¿è¡Œå³å¯å¾—åˆ°è½¬æ¢ç»“æœã€‚æ³¨æ„ï¼Œ**`OUTPUT_MD_PATH`å¿…é¡»ä»¥mdä¸ºç»“å°¾ï¼Œä¸èƒ½ä»¥æ–‡ä»¶å¤¹ç»“å°¾ï¼Œä¾‹å¦‚`Dï¼š...\pdf_output\A2500836.md`ï¼Œä½†ä½ ä¸å¿…æå‰åˆ›å»ºæ­¤æ–‡ä»¶**

è‹¥ä½ æƒ³æ‰¹é‡ä¸€æ¬¡æ€§å¤„ç†å®Œ`pdf_input`ä¸‹çš„æ‰€æœ‰pdfå¹¶è½¬ä¸ºå¯¹åº”åç§°çš„mdæ–‡æ¡£ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼š
```python
import os
import io
import fitz  # PyMuPDF
import torch
import sys
import contextlib
import time
from PIL import Image
from transformers import AutoModel, AutoTokenizer

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ =================

# 1. è¾“å…¥æ–‡ä»¶å¤¹ (å­˜æ”¾æ‰€æœ‰ PDF çš„ç›®å½•)
INPUT_DIR = r'D:\..\deepseek-ocr2\DeepSeek-OCR-2\pdf_input'#æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 2. è¾“å‡ºæ–‡ä»¶å¤¹ (å­˜æ”¾ç”Ÿæˆ MD çš„ç›®å½•)
OUTPUT_DIR = r'D:\..\deepseek-ocr2\DeepSeek-OCR-2\pdf_output'#æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 3. æ¨¡å‹è·¯å¾„
MODEL_PATH = r'D:\ModelScope_Cache\models\deepseek-ai\DeepSeek-OCR-2'#æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# ===============================================

def clean_ocr_output(text):
    """ æ¸…æ´—å™¨ï¼šå»é™¤æ¨¡å‹è¾“å‡ºä¸­çš„åƒåœ¾æ—¥å¿— """
    lines = text.split('\n')
    clean_lines = []
    # å®šä¹‰åƒåœ¾æ—¥å¿—ç‰¹å¾
    junk_keywords = [
        "The attention mask", "Setting `pad_token_id`", "The `seen_tokens` attribute",
        "BASE:  torch.Size", "PATCHES:  torch.Size", "UserWarning:", 
        "configuration_utils.py", "modeling_deepseekocr2.py", "warnings.warn",
        "Loading checkpoint", "input_ids", "attention_mask", "position_ids",
        "Using `bitsandbytes`", "was not set", "cannot be inferred",
        "Using the slow distutils backend", "get_max_cache", "transitioning from computing"
    ]

    for line in lines:
        is_junk = False
        for kw in junk_keywords:
            if kw in line:
                is_junk = True
                break
        if not is_junk:
            clean_lines.append(line)
            
    return '\n'.join(clean_lines).strip()

def load_pdf_as_images(pdf_path, dpi=144):
    images = []
    try:
        pdf_document = fitz.open(pdf_path)
        zoom = dpi / 72.0
        matrix = fitz.Matrix(zoom, zoom)
        for page_num in range(pdf_document.page_count):
            page = pdf_document[page_num]
            pixmap = page.get_pixmap(matrix=matrix, alpha=False)
            img = Image.open(io.BytesIO(pixmap.tobytes("png"))).convert("RGB")
            images.append(img)
        pdf_document.close()
        return images
    except Exception as e:
        print(f"Error loading PDF {pdf_path}: {e}")
        return []

def process_one_pdf(pdf_path, model, tokenizer):
    """ å¤„ç†å•ä¸ª PDF æ–‡ä»¶çš„æ ¸å¿ƒé€»è¾‘ """
    
    # 1. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    file_name = os.path.basename(pdf_path) # ä¾‹å¦‚: 2500836.pdf
    file_base_name = os.path.splitext(file_name)[0] # ä¾‹å¦‚: 2500836
    output_md_path = os.path.join(OUTPUT_DIR, f"{file_base_name}.md")
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤è·‘ (å¯é€‰)
    # if os.path.exists(output_md_path):
    #     print(f"   [è·³è¿‡] æ–‡ä»¶å·²å­˜åœ¨: {file_name}")
    #     return

    print(f"\nğŸ“„ æ­£åœ¨å¤„ç†: {file_name} -> {file_base_name}.md")
    
    # 2. è½¬å›¾ç‰‡
    images = load_pdf_as_images(pdf_path)
    if not images:
        print("   [é”™è¯¯] PDFåŠ è½½å¤±è´¥æˆ–ä¸ºç©ºã€‚")
        return

    full_content = f"# {file_base_name}\n\n"
    prompt = "<image>\nFree OCR."
    
    # 3. é€é¡µè¯†åˆ«
    start_time = time.time()
    for i, img in enumerate(images):
        print(f"   - Page {i+1}/{len(images)} ... ", end="", flush=True)
        
        temp_img_path = f"temp_{file_base_name}_{i}.png"
        img.save(temp_img_path)
        
        captured_output = io.StringIO()
        try:
            with contextlib.redirect_stdout(captured_output):
                with contextlib.redirect_stderr(captured_output):
                    model.infer(
                        tokenizer, 
                        prompt=prompt, 
                        image_file=temp_img_path, 
                        output_path="./temp_dummy", 
                        base_size=1024, 
                        image_size=768, 
                        crop_mode=True, 
                        save_results=False
                    )
            
            raw_text = captured_output.getvalue()
            clean_text = clean_ocr_output(raw_text)
            
            if clean_text:
                full_content += f"\n\n<!-- Page {i+1} -->\n\n" + clean_text
                print(f"OK ({len(clean_text)} chars)")
            else:
                print("Warning (Empty)")
                
        except Exception as e:
            print(f"Error: {e}")
        finally:
            captured_output.close()
            if os.path.exists(temp_img_path):
                try: os.remove(temp_img_path)
                except: pass

    # 4. ä¿å­˜ç»“æœ
    try:
        with open(output_md_path, "w", encoding="utf-8") as f:
            f.write(full_content)
        elapsed = time.time() - start_time
        print(f"   âœ… å®Œæˆï¼è€—æ—¶: {elapsed:.1f}s, ä¿å­˜è‡³: {output_md_path}")
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")

def main():
    # 0. åˆå§‹åŒ–
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("ğŸ¤– [1/3] æ­£åœ¨åŠ è½½æ¨¡å‹ (DeepSeek-OCR2)...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    try:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, _attn_implementation='flash_attention_2')
        print("   -> Flash Attention: ON (åŠ é€Ÿæ¨¡å¼)")
    except:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True)
        print("   -> Flash Attention: OFF (æ™®é€šæ¨¡å¼)")

    model = model.eval().cuda().to(torch.bfloat16)

    # 1. æ‰«ææ–‡ä»¶
    print(f"\nğŸ“‚ [2/3] æ‰«æè¾“å…¥ç›®å½•: {INPUT_DIR}")
    pdf_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith('.pdf')]
    
    if not pdf_files:
        print("   æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶ï¼è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return
    
    print(f"   æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶ï¼Œå‡†å¤‡å¼€å§‹å¤„ç†ã€‚")

    # 2. æ‰¹é‡å¾ªç¯
    print("\nğŸš€ [3/3] å¼€å§‹æ‰¹é‡è½¬æ¢ä»»åŠ¡...")
    for idx, pdf_file in enumerate(pdf_files):
        print(f"\n[{idx+1}/{len(pdf_files)}] ä»»åŠ¡å¯åŠ¨ --------------------------")
        pdf_full_path = os.path.join(INPUT_DIR, pdf_file)
        process_one_pdf(pdf_full_path, model, tokenizer)
        
    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()
```
>tips:è¯·ç¡®ä¿ä½ çš„ `INPUT_DIR` è·¯å¾„é‡ŒçœŸçš„æœ‰ PDF æ–‡ä»¶ã€‚

## å›¾ç‰‡è½¬md
æ‰“å¼€`DeepSeek-OCR2-hf`ä¸‹çš„`run_dpsk_ocr2.py`,åœ¨`model_name`ã€`image_file`ã€`output_path`ä¸‹å¡«å…¥ä½ å…·ä½“çš„è·¯å¾„ï¼Œæ³¨æ„ä½¿ç”¨`r`è½¬ä¹‰ï¼Œæ­¤`output_path`ä»¥æ–‡ä»¶å¤¹`img_output`ç»“å°¾å³å¯ã€‚è‹¥ä½ æƒ³æ‰¹é‡å¤„ç†`img_input`ä¸‹çš„å›¾ç‰‡ï¼Œå¯å‚è€ƒä¸‹é¢çš„ä»£ç ï¼Œæä¾›äº†ä¸€å¯¹ä¸€mdå’Œåˆå¹¶mdçš„ç‰ˆæœ¬ï¼š
```python
import os
import io
import torch
import sys
import contextlib
import time
from transformers import AutoModel, AutoTokenizer

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ =================

# 1. è¾“å…¥æ–‡ä»¶å¤¹ (å­˜æ”¾å›¾ç‰‡çš„ç›®å½•)
INPUT_DIR = r'D:\..\deepseek-ocr2\DeepSeek-OCR-2\img_input' #æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 2. è¾“å‡ºæ–‡ä»¶å¤¹ (å­˜æ”¾ç”Ÿæˆ MD çš„ç›®å½•)
OUTPUT_DIR = r'D:\..\deepseek-ocr2\DeepSeek-OCR-2\img_output' #æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 3. æ¨¡å‹è·¯å¾„
MODEL_PATH = r'D:\ModelScope_Cache\models\deepseek-ai\DeepSeek-OCR-2'#æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 4. æç¤ºè¯
PROMPT = "<image>\n<|grounding|>Convert the document to markdown."

# ===============================================

def clean_ocr_output(text):
    """ æ¸…æ´—å™¨ï¼šå»é™¤åƒåœ¾æ—¥å¿— """
    lines = text.split('\n')
    clean_lines = []
    junk_keywords = [
        "The attention mask", "Setting `pad_token_id`", "The `seen_tokens` attribute",
        "BASE:  torch.Size", "PATCHES:  torch.Size", "UserWarning:", 
        "configuration_utils.py", "modeling_deepseekocr2.py", "warnings.warn",
        "Loading checkpoint", "input_ids", "attention_mask", "position_ids",
        "get_max_cache", "transitioning from computing"
    ]
    for line in lines:
        if not any(kw in line for kw in junk_keywords):
            clean_lines.append(line)
    return '\n'.join(clean_lines).strip()

def process_batch_images(model, tokenizer):
    """ æ‰¹é‡å¤„ç†å›¾ç‰‡å¹¶åˆ†åˆ«ä¿å­˜
        è‡ªåŠ¨è¯»å– INPUT_DIR ä¸‹çš„ .jpg, .png ç­‰æ ¼å¼çš„å›¾ç‰‡
        å¦‚æœè¾“å…¥æ˜¯ img_01.pngï¼Œå®ƒä¼šè‡ªåŠ¨ä¿å­˜ä¸º img_01.md
    """
    
    # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.webp', '.tiff')
    
    # æ‰«ææ–‡ä»¶
    image_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(valid_extensions)]
    # æŒ‰æ–‡ä»¶åæ’åºï¼Œä¿è¯å¤„ç†é¡ºåº
    image_files.sort() 
    
    if not image_files:
        print(f"âŒ åœ¨ {INPUT_DIR} æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ã€‚")
        return

    print(f"ğŸš€ æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    for idx, img_name in enumerate(image_files):
        img_path = os.path.join(INPUT_DIR, img_name)
        file_base_name = os.path.splitext(img_name)[0]
        output_md_path = os.path.join(OUTPUT_DIR, f"{file_base_name}.md")
        
        print(f"[{idx+1}/{len(image_files)}] å¤„ç†: {img_name} ... ", end="", flush=True)

        captured_output = io.StringIO()
        try:
            # æ ¸å¿ƒï¼šæ‹¦æˆªæ§åˆ¶å°è¾“å‡º
            with contextlib.redirect_stdout(captured_output):
                with contextlib.redirect_stderr(captured_output):
                    model.infer(
                        tokenizer, 
                        prompt=PROMPT, 
                        image_file=img_path, 
                        output_path="./temp_dummy", # è¿™é‡Œçš„è·¯å¾„ä¸é‡è¦
                        base_size=1024, 
                        image_size=768, 
                        crop_mode=True, 
                        save_results=False
                    )
            
            # æ¸…æ´—æ•°æ®
            raw_text = captured_output.getvalue()
            clean_text = clean_ocr_output(raw_text)
            
            # ä¿å­˜å•ä¸ªMD
            if clean_text:
                with open(output_md_path, "w", encoding="utf-8") as f:
                    f.write(clean_text)
                print(f"âœ… å·²ä¿å­˜ -> {file_base_name}.md")
            else:
                print("âš ï¸ å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜ã€‚")
                
        except Exception as e:
            print(f"âŒ å¤±è´¥: {e}")
        finally:
            captured_output.close()

def merge_markdown_files(merged_file_name="merged_result.md"):
    """ 
    åˆå¹¶æ¥å£ï¼šå°† OUTPUT_DIR ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ª 
    å°è¯•æ ¹æ®æ–‡ä»¶åé‡Œçš„æ•°å­—è¿›è¡Œæ’åºã€‚ä¾‹å¦‚ï¼Œå®ƒèƒ½æ­£ç¡®åœ°æŠŠ page_2.md æ’åœ¨ page_10.md å‰é¢ã€‚
    åˆå¹¶åçš„æ–‡ä»¶ä¼šä¿å­˜åœ¨ OUTPUT_DIR ä¸‹ï¼Œåå­—å« all_images_merged.mdï¼ˆä½ å¯ä»¥åœ¨ main å‡½æ•°é‡Œæ”¹åï¼‰ã€‚
    """
    print(f"\nğŸ”— æ­£åœ¨åˆå¹¶æ‰€æœ‰ Markdown æ–‡ä»¶...")
    
    # è·å–æ‰€æœ‰mdæ–‡ä»¶å¹¶æ’åº
    md_files = [f for f in os.listdir(OUTPUT_DIR) if f.lower().endswith('.md')]
    # è¿‡æ»¤æ‰å¯èƒ½å­˜åœ¨çš„åˆå¹¶ç»“æœæ–‡ä»¶æœ¬èº«ï¼Œé˜²æ­¢é€’å½’
    md_files = [f for f in md_files if f != merged_file_name]
    
    # æ™ºèƒ½æ’åºï¼šå°è¯•æŒ‰æ•°å­—é¡ºåºæ’åº (æ¯”å¦‚ 1.md, 2.md, 10.md)ï¼Œè€Œä¸æ˜¯é»˜è®¤çš„å­—ç¬¦é¡ºåº (1.md, 10.md, 2.md)
    try:
        md_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x))) if any(char.isdigit() for char in x) else x)
    except:
        md_files.sort() # å¦‚æœæ–‡ä»¶åä¸å«æ•°å­—ï¼Œå›é€€åˆ°é»˜è®¤æ’åº

    if not md_files:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„ MD æ–‡ä»¶ã€‚")
        return

    merged_path = os.path.join(OUTPUT_DIR, merged_file_name)
    
    with open(merged_path, 'w', encoding='utf-8') as outfile:
        outfile.write(f"# Merged Document\n\nGenerated on {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        for md_file in md_files:
            file_path = os.path.join(OUTPUT_DIR, md_file)
            with open(file_path, 'r', encoding='utf-8') as infile:
                content = infile.read()
                
                # æ·»åŠ æ–‡ä»¶åˆ†å‰²æ ‡è¯†
                outfile.write(f"\n\n--- Source: {md_file} ---\n\n")
                outfile.write(content)
    
    print(f"ğŸ‰ åˆå¹¶å®Œæˆï¼æ€»æ–‡ä»¶å·²ä¿å­˜è‡³:\n{merged_path}")

def main():
    # 0. ç¯å¢ƒè®¾ç½®
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'
    
    # 1. åŠ è½½æ¨¡å‹
    print("ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    try:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, _attn_implementation='flash_attention_2')
    except:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True)
    
    model = model.eval().cuda().to(torch.bfloat16)

    # 2. æ‰§è¡Œæ‰¹é‡å¤„ç†
    process_batch_images(model, tokenizer)

    # 3. æ‰§è¡Œåˆå¹¶ (å¦‚æœä½ ä¸æƒ³åˆå¹¶ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ)
    merge_markdown_files(merged_file_name="all_images_merged.md")

if __name__ == "__main__":
    main()
```
# æœ¬åœ°å¿«æ·åº”ç”¨å°è£…
## ç•Œé¢
å®‰è£…å›¾å½¢ç•Œé¢åº“ï¼š
```dsocr2
pip install PyQt5
```
åœ¨`DeepSeek-OCR2-master`ä¸‹æ–°å»ºä¸€ä¸ªæ–‡ä»¶ `app_gui.py`ï¼Œå°†ä»¥ä¸‹ä»£ç å…¨éƒ¨å¤åˆ¶è¿›å»ï¼ŒåŒ…å«äº†ä»¥ä¸‹åŠŸèƒ½ï¼š
- æ–‡ä»¶åˆ—è¡¨ï¼šæ”¯æŒæ‹–æ‹½æˆ–æŒ‰é’®æ·»åŠ å›¾ç‰‡/PDFï¼Œæ··åˆæ’åˆ—ã€‚
- çµæ´»åˆå¹¶ï¼šå¯ä»¥é€‰æ‹©â€œåˆå¹¶æ‰€æœ‰â€æˆ–â€œå•ç‹¬ä¿å­˜â€ã€‚
- å®æ—¶æ—¥å¿—ï¼šç•Œé¢ä¸‹æ–¹æœ‰æ—¥å¿—çª—å£ï¼Œæ˜¾ç¤ºè½¬æ¢è¿›åº¦ã€‚
- é…ç½®é¡¹ï¼šå¯ä»¥æ‰‹åŠ¨ä¿®æ”¹æ¨¡å‹è·¯å¾„ã€æ–‡ä»¶ä¿å­˜è·¯å¾„ã€‚
```python
import sys
import os
import io
import time
import contextlib
import threading
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout, 
                             QPushButton, QListWidget, QFileDialog, QLabel, QCheckBox, 
                             QTextEdit, QProgressBar, QLineEdit, QGroupBox, QSplitter, QMessageBox)
from PyQt5.QtCore import Qt, QThread, pyqtSignal
from PyQt5.QtGui import QIcon, QFont

# å¼•å…¥æ ¸å¿ƒå¤„ç†é€»è¾‘éœ€è¦çš„åº“
import fitz  # PyMuPDF
from PIL import Image
import torch
from transformers import AutoModel, AutoTokenizer

# ================= åç«¯å¤„ç†çº¿ç¨‹ =================

class OCRWorker(QThread):
    log_signal = pyqtSignal(str)       # å‘é€æ—¥å¿—ä¿¡å·
    progress_signal = pyqtSignal(int)  # å‘é€è¿›åº¦ä¿¡å·
    finish_signal = pyqtSignal()       # å®Œæˆä¿¡å·

    def __init__(self, file_list, model_path, output_dir, merge_mode, output_filename):
        super().__init__()
        self.file_list = file_list
        self.model_path = model_path
        self.output_dir = output_dir
        self.merge_mode = merge_mode # True: åˆå¹¶, False: ä¸åˆå¹¶
        self.output_filename = output_filename
        self.model = None
        self.tokenizer = None
        self.stop_flag = False

    def clean_ocr_output(self, text):
        lines = text.split('\n')
        clean_lines = []
        junk_keywords = [
            "The attention mask", "Setting `pad_token_id`", "BASE:  torch.Size", 
            "PATCHES:  torch.Size", "UserWarning:", "configuration_utils.py", 
            "modeling_deepseekocr2.py", "warnings.warn", "Loading checkpoint", 
            "input_ids", "attention_mask", "position_ids", "get_max_cache"
        ]
        for line in lines:
            if not any(kw in line for kw in junk_keywords):
                clean_lines.append(line)
        return '\n'.join(clean_lines).strip()

    def load_model(self):
        self.log_signal.emit("ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹ (DeepSeek-OCR2)... è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            try:
                self.model = AutoModel.from_pretrained(self.model_path, trust_remote_code=True, _attn_implementation='flash_attention_2')
                self.log_signal.emit("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (Flash Attention åŠ é€Ÿå¼€å¯)")
            except:
                self.model = AutoModel.from_pretrained(self.model_path, trust_remote_code=True)
                self.log_signal.emit("âš ï¸ æ¨¡å‹åŠ è½½æˆåŠŸ (Flash Attention æœªå®‰è£…ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼)")
            
            self.model = self.model.eval().cuda().to(torch.bfloat16)
            return True
        except Exception as e:
            self.log_signal.emit(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False

    def run(self):
        if not self.load_model():
            return

        total_files = len(self.file_list)
        all_md_content = f"# OCR Result\n\nDate: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        prompt = "<image>\nFree OCR."
        
        for idx, file_path in enumerate(self.file_list):
            if self.stop_flag: break
            
            file_name = os.path.basename(file_path)
            self.log_signal.emit(f"\nğŸ“„ [{idx+1}/{total_files}] æ­£åœ¨å¤„ç†: {file_name}")
            
            # è¯†åˆ«é€»è¾‘
            file_content = ""
            images = []

            try:
                # 1. åˆ¤æ–­ç±»å‹å¹¶è½¬å›¾ç‰‡
                if file_path.lower().endswith('.pdf'):
                    pdf_doc = fitz.open(file_path)
                    for p_num in range(pdf_doc.page_count):
                        pix = pdf_doc[p_num].get_pixmap(matrix=fitz.Matrix(2, 2))
                        img = Image.open(io.BytesIO(pix.tobytes("png"))).convert("RGB")
                        images.append(img)
                    pdf_doc.close()
                else:
                    # å›¾ç‰‡
                    img = Image.open(file_path).convert("RGB")
                    images.append(img)
                
                # 2. é€å›¾æ¨ç†
                file_content += f"# File: {file_name}\n\n"
                
                for i, img in enumerate(images):
                    self.log_signal.emit(f"   -> å¤„ç†é¡µ/å›¾ {i+1}/{len(images)}...")
                    
                    temp_path = f"temp_gui_{i}.png"
                    img.save(temp_path)
                    
                    captured_output = io.StringIO()
                    with contextlib.redirect_stdout(captured_output):
                         with contextlib.redirect_stderr(captured_output):
                            self.model.infer(self.tokenizer, prompt=prompt, image_file=temp_path, 
                                            output_path="./dummy", base_size=1024, image_size=768, 
                                            crop_mode=True, save_results=False)
                    
                    raw_text = captured_output.getvalue()
                    clean_text = self.clean_ocr_output(raw_text)
                    
                    if clean_text:
                        file_content += f"\n\n<!-- Page/Part {i+1} -->\n\n{clean_text}"
                    
                    if os.path.exists(temp_path): os.remove(temp_path)

                # 3. å¤„ç†ç»“æœ
                if self.merge_mode:
                    all_md_content += f"\n\n---\n\n{file_content}"
                else:
                    # å•ç‹¬ä¿å­˜
                    single_out_path = os.path.join(self.output_dir, os.path.splitext(file_name)[0] + ".md")
                    with open(single_out_path, "w", encoding="utf-8") as f:
                        f.write(file_content)
                    self.log_signal.emit(f"   âœ… å·²ä¿å­˜è‡³: {single_out_path}")

            except Exception as e:
                self.log_signal.emit(f"âŒ å¤„ç†å‡ºé”™: {str(e)}")

            # æ›´æ–°è¿›åº¦æ¡
            progress = int((idx + 1) / total_files * 100)
            self.progress_signal.emit(progress)

        # å¦‚æœæ˜¯åˆå¹¶æ¨¡å¼ï¼Œæœ€åä¿å­˜å¤§æ–‡ä»¶
        if self.merge_mode and not self.stop_flag:
            merge_path = os.path.join(self.output_dir, self.output_filename)
            with open(merge_path, "w", encoding="utf-8") as f:
                f.write(all_md_content)
            self.log_signal.emit(f"\nğŸ‰ åˆå¹¶æ–‡ä»¶å·²ä¿å­˜è‡³: {merge_path}")
        
        self.finish_signal.emit()

# ================= å‰ç«¯ç•Œé¢é€»è¾‘ =================

class OCRApp(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("DeepSeek-OCR2 æ¡Œé¢åŠ©æ‰‹")
        self.resize(900, 700)
        self.initUI()
        self.worker = None

    def initUI(self):
        main_widget = QWidget()
        self.setCentralWidget(main_widget)
        layout = QVBoxLayout(main_widget)

        # 1. é¡¶éƒ¨é…ç½®åŒº
        config_group = QGroupBox("é…ç½®ä¸è·¯å¾„")
        config_layout = QVBoxLayout()
        
        # æ¨¡å‹è·¯å¾„
        h_layout_model = QHBoxLayout()
        h_layout_model.addWidget(QLabel("æ¨¡å‹è·¯å¾„:"))
        self.model_path_edit = QLineEdit(r"D:\ModelScope_Cache\models\deepseek-ai\DeepSeek-OCR-2")#æ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ¨¡å‹è·¯å¾„
        h_layout_model.addWidget(self.model_path_edit)
        config_layout.addLayout(h_layout_model)

        # è¾“å‡ºè·¯å¾„
        h_layout_out = QHBoxLayout()
        h_layout_out.addWidget(QLabel("è¾“å‡ºç›®å½•:"))
        self.out_path_edit = QLineEdit(os.getcwd()) # é»˜è®¤å½“å‰ç›®å½•ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ç›®å½•
        btn_sel_out = QPushButton("é€‰æ‹©")
        btn_sel_out.clicked.connect(self.select_output_dir)
        h_layout_out.addWidget(self.out_path_edit)
        h_layout_out.addWidget(btn_sel_out)
        config_layout.addLayout(h_layout_out)
        
        config_group.setLayout(config_layout)
        layout.addWidget(config_group)

        # 2. ä¸­é—´æ–‡ä»¶åˆ—è¡¨åŒº
        self.file_list_widget = QListWidget()
        self.file_list_widget.setAcceptDrops(True)
        self.file_list_widget.setDragDropMode(QListWidget.InternalMove) # å…è®¸æ‹–æ‹½æ’åº
        
        # å¯ç”¨æ–‡ä»¶æ‹–å…¥
        self.file_list_widget.dragEnterEvent = self.dragEnterEvent
        self.file_list_widget.dragMoveEvent = self.dragMoveEvent
        self.file_list_widget.dropEvent = self.dropEvent
        
        btn_layout = QHBoxLayout()
        btn_add_files = QPushButton("æ·»åŠ æ–‡ä»¶ (PDF/å›¾ç‰‡)")
        btn_add_files.clicked.connect(self.add_files)
        btn_clear = QPushButton("æ¸…ç©ºåˆ—è¡¨")
        btn_clear.clicked.connect(self.file_list_widget.clear)
        btn_layout.addWidget(btn_add_files)
        btn_layout.addWidget(btn_clear)

        layout.addWidget(QLabel("å¾…å¤„ç†æ–‡ä»¶ (æ”¯æŒæ‹–æ‹½æ’åº/æ·»åŠ ):"))
        layout.addWidget(self.file_list_widget)
        layout.addLayout(btn_layout)

        # 3. åº•éƒ¨æ“ä½œåŒº
        op_group = QGroupBox("æ“ä½œé€‰é¡¹")
        op_layout = QHBoxLayout()
        
        self.chk_merge = QCheckBox("å°†ç»“æœåˆå¹¶ä¸ºä¸€ä¸ªæ–‡ä»¶")
        self.merge_name_edit = QLineEdit("merged_result.md")
        self.merge_name_edit.setPlaceholderText("åˆå¹¶æ–‡ä»¶å")
        self.merge_name_edit.setEnabled(False)
        self.chk_merge.toggled.connect(lambda: self.merge_name_edit.setEnabled(self.chk_merge.isChecked()))

        self.btn_start = QPushButton("å¼€å§‹è½¬æ¢")
        self.btn_start.setFixedHeight(40)
        self.btn_start.setStyleSheet("background-color: #4CAF50; color: white; font-weight: bold;")
        self.btn_start.clicked.connect(self.start_ocr)

        op_layout.addWidget(self.chk_merge)
        op_layout.addWidget(self.merge_name_edit)
        op_layout.addStretch()
        op_layout.addWidget(self.btn_start)
        op_group.setLayout(op_layout)
        layout.addWidget(op_group)

        # 4. è¿›åº¦ä¸æ—¥å¿—
        self.progress_bar = QProgressBar()
        self.log_text = QTextEdit()
        self.log_text.setReadOnly(True)
        
        layout.addWidget(self.progress_bar)
        layout.addWidget(QLabel("è¿è¡Œæ—¥å¿—:"))
        layout.addWidget(self.log_text)

    # --- äº‹ä»¶å¤„ç† ---
    def select_output_dir(self):
        dir_path = QFileDialog.getExistingDirectory(self, "é€‰æ‹©è¾“å‡ºç›®å½•")
        if dir_path:
            self.out_path_edit.setText(dir_path)

    def add_files(self):
        files, _ = QFileDialog.getOpenFileNames(self, "é€‰æ‹©æ–‡ä»¶", "", "Files (*.pdf *.jpg *.jpeg *.png *.bmp)")
        if files:
            self.file_list_widget.addItems(files)

    def dragEnterEvent(self, event):
        if event.mimeData().hasUrls():
            event.accept()
        else:
            event.ignore()

    def dragMoveEvent(self, event):
        if event.mimeData().hasUrls():
            event.setDropAction(Qt.CopyAction)
            event.accept()
        else:
            event.ignore()

    def dropEvent(self, event):
        if event.mimeData().hasUrls():
            event.setDropAction(Qt.CopyAction)
            event.accept()
            links = []
            for url in event.mimeData().urls():
                links.append(str(url.toLocalFile()))
            self.file_list_widget.addItems(links)
        else:
            event.ignore()

    def log(self, text):
        self.log_text.append(text)
        self.log_text.verticalScrollBar().setValue(self.log_text.verticalScrollBar().maximum())

    def start_ocr(self):
        # æ”¶é›†æ–‡ä»¶
        files = [self.file_list_widget.item(i).text() for i in range(self.file_list_widget.count())]
        if not files:
            QMessageBox.warning(self, "æç¤º", "è¯·å…ˆæ·»åŠ æ–‡ä»¶ï¼")
            return

        model_path = self.model_path_edit.text()
        output_dir = self.out_path_edit.text()
        merge_mode = self.chk_merge.isChecked()
        merge_name = self.merge_name_edit.text()

        # é”å®šç•Œé¢
        self.btn_start.setEnabled(False)
        self.file_list_widget.setEnabled(False)
        self.progress_bar.setValue(0)
        self.log_text.clear()

        # å¯åŠ¨çº¿ç¨‹
        self.worker = OCRWorker(files, model_path, output_dir, merge_mode, merge_name)
        self.worker.log_signal.connect(self.log)
        self.worker.progress_signal.connect(self.progress_bar.setValue)
        self.worker.finish_signal.connect(self.on_finish)
        self.worker.start()

    def on_finish(self):
        self.btn_start.setEnabled(True)
        self.file_list_widget.setEnabled(True)
        QMessageBox.information(self, "å®Œæˆ", "æ‰€æœ‰ä»»åŠ¡å·²å¤„ç†å®Œæ¯•ï¼")

if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = OCRApp()
    window.show()
    sys.exit(app.exec_())
```
## å¿«æ·è¿è¡Œ
åœ¨ `app_gui.py` åŒçº§ç›®å½•ä¸‹ï¼Œæ–°å»ºä¸€ä¸ªæ–‡æœ¬æ–‡æ¡£ï¼Œé‡å‘½åä¸º `start_ocr.bat`ï¼Œè¾“å…¥ä»¥ä¸‹å†…å®¹ï¼š
```BATCH
@echo off
chcp 65001
"C:\Users\...\anaconda3\envs\dsocr2\python.exe" "D:\..\deepseek-ocr2\DeepSeek-OCR-2\DeepSeek-OCR2-master\app_gui.py"
pause
```
è¯·å°†ä¸Šè¿°`C:\Users\...\anaconda3\envs\dsocr2\python.exe`æ›¿æ¢ä¸ºè‡ªå·±ç”µè„‘çš„ç”¨æˆ·è·¯å¾„ï¼Œ`D:\..\deepseek-ocr2\DeepSeek-OCR-2\app_gui.py`æ›¿æ¢ä¸ºè‡ªå·±çš„`app_gui.py`è·¯å¾„ã€‚æ³¨æ„ï¼Œ`chcp 65001`æ˜¯ä¸ºäº†è§£å†³è·¯å¾„ä¸­å­˜åœ¨ä¸­æ–‡å­—ç¬¦è€Œä½¿ç”¨çš„ã€‚

åŒå‡»è¿™ä¸ª `start_ocr.bat`ï¼Œçœ‹çœ‹èƒ½ä¸èƒ½è‡ªåŠ¨å¼¹å‡ºè½¯ä»¶ç•Œé¢ã€‚å¦‚æœèƒ½ï¼Œç»§ç»­ä¸‹ä¸€æ­¥ã€‚

å³é”®ç‚¹å‡» `start_ocr.bat` -> å‘é€åˆ° -> æ¡Œé¢å¿«æ·æ–¹å¼ã€‚

å›åˆ°æ¡Œé¢ï¼Œå³é”®ç‚¹å‡»åˆšæ‰ç”Ÿæˆçš„å¿«æ·æ–¹å¼ -> å±æ€§ã€‚ç‚¹å‡» â€œæ›´æ”¹å›¾æ ‡â€ï¼Œé€‰ä¸€ä¸ªå¥½çœ‹çš„å›¾æ ‡ã€‚åœ¨ â€œè¿è¡Œæ–¹å¼â€ é‡Œé€‰æ‹© â€œæœ€å°åŒ–â€ï¼ˆè¿™æ ·å¯åŠ¨æ—¶é‚£ä¸ªé»‘è‰²çš„ CMD çª—å£å°±ä¼šé—ªä¸€ä¸‹æ¶ˆå¤±ï¼Œä¸ä¼šä¸€ç›´æŒ¡ç€ï¼‰ã€‚å¹¶å°†åå­—æ”¹ä¸º`DeepSeek-OCR2`ã€‚


