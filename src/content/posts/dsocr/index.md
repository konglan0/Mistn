---
title: winç³»ç»Ÿä¸‹DeepSeek-OCR2éƒ¨ç½²ä¸ä½¿ç”¨
published: 2026-01-30
tags: [DeepSeek-OCR2]
category: å¼€æºéƒ¨ç½²
image: "./9.webp"
draft: false
licenseName: "CC BY-NC-SA 4.0"
---
# æ¨¡å‹ä¸‹è½½
## è®¾ç½®é­”å¡”ç¤¾åŒºï¼ˆModelScopeï¼‰ä¸‹è½½è·¯å¾„
é…ç½®ç¯å¢ƒå˜é‡ï¼Œå°†é­”å¡”ç¤¾åŒºä¿å­˜æ¨¡å‹çš„è·¯å¾„ä»é»˜è®¤çš„Cç›˜è°ƒæ•´è‡³Dç›˜ã€‚æŒ‰ç…§å¦‚ä¸‹æ­¥éª¤è¿›è¡Œï¼š
- åœ¨ Windows æœç´¢æ è¾“å…¥â€œç¼–è¾‘ç³»ç»Ÿç¯å¢ƒå˜é‡â€ï¼Œç‚¹å‡»â€œç¯å¢ƒå˜é‡â€æŒ‰é’®ã€‚
- åœ¨â€œç”¨æˆ·å˜é‡â€æˆ–â€œç³»ç»Ÿå˜é‡â€åŒºåŸŸï¼Œç‚¹å‡»â€œæ–°å»ºâ€ã€‚
    - å˜é‡å: `MODELSCOPE_CACHE`
    - å˜é‡å€¼: `D:\ModelScope_Cache` (æˆ–è€…æ˜¯ä½ æƒ³è¦å­˜æ”¾æ¨¡å‹çš„ D ç›˜å…·ä½“è·¯å¾„)
    - ä¿å­˜å¹¶é‡å¯ä½ çš„ç»ˆç«¯ï¼ˆCMD/PowerShellï¼‰æˆ– æ‰€æœ‰çš„IDEï¼ˆVS Code/PyCharmï¼‰
- æ–°å»ºæ–‡ä»¶å¤¹ModelScope_Cacheä¸deepseek-OCR2ï¼Œåœ¨deepseek-OCR2æ–‡ä»¶å¤¹å†…æ‰“å¼€VS codeï¼ˆIDEï¼‰
## ä¸‹è½½æ¨¡å‹æƒé‡
åœ¨VS codeä¸­æ–°å»ºé»˜è®¤ç»ˆç«¯ï¼Œåˆ›å»ºæ–°ç¯å¢ƒï¼š
```base
conda create -n dsocr2 python=3.12.9 -y
conda activate dsocr2
```
æ¿€æ´»ç¯å¢ƒåï¼Œå®‰è£…é­”å¡”ç¤¾åŒºåŒ…å¹¶ä¸‹è½½æ¨¡å‹æƒé‡ï¼š
```dsocr2
pip install modelscope
modelscope download --model deepseek-ai/DeepSeek-OCR-2
```
## ä¸‹è½½æ¨¡å‹æºç 
ä½¿ç”¨gitå…‹éš†æºç å¹¶è¿›å…¥é¡¹ç›®æ–‡ä»¶å¤¹ï¼š
```dsocr2
git clone https://github.com/deepseek-ai/DeepSeek-OCR-2.git
cd DeepSeek-OCR-2
```
ä¾æ¬¡å®‰è£…å¿…å¤‡åŒ…ï¼š
```dsocr2
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 =2.6.0 --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt

```
:::note[æ¨¡å‹å­˜æ”¾Dç›˜çš„ç›¸å…³å¯¼å…¥æ³¨æ„äº‹é¡¹]
**1.æƒ…å†µä¸€ï¼šä½¿ç”¨ ModelScope å®˜æ–¹ SDK åŠ è½½ï¼ˆæœ€çœäº‹ï¼‰**

å¦‚æœä½ åœ¨ä»£ç ä¸­ä½¿ç”¨ `snapshot_download` æˆ– `AutoModel` æ¥åŠ è½½æ¨¡å‹ï¼Œåªè¦ä½ è®¾ç½®äº†ç¯å¢ƒå˜é‡ `MODELSCOPE_CACHE`ï¼Œä»£ç ä¼šè‡ªåŠ¨å»æ–°è·¯å¾„æ‰¾æ¨¡å‹ï¼Œä¸éœ€è¦æ”¹ä»£ç ã€‚
```python
import os
# å¦‚æœä½ æ²¡æœ‰åœ¨ç³»ç»Ÿå±‚é¢è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¯ä»¥åœ¨ä»£ç æœ€å¼€å¤´æ‰‹åŠ¨æŒ‡å®š
# os.environ['MODELSCOPE_CACHE'] = 'D:\\ModelScope_Cache' 

from modelscope import snapshot_download
# è¿™è¡Œä»£ç ä¼šè‡ªåŠ¨å» D:\ModelScope_Cache æŸ¥æ‰¾ï¼Œæ‰¾ä¸åˆ°æ‰ä¼šä¸‹è½½
model_dir = snapshot_download('deepseek-ai/DeepSeek-OCR-2') 
```
**2.æƒ…å†µäºŒï¼šæ‰‹åŠ¨æŒ‡å®šç»å¯¹è·¯å¾„**

å¦‚æœä½ ä¸æƒ³ä¾èµ–ç¯å¢ƒå˜é‡ï¼Œä½ éœ€è¦çŸ¥é“æ¨¡å‹ä¸‹è½½åçš„å…·ä½“æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¹¶åœ¨ä»£ç ä¸­æ˜¾å¼æŒ‡å®šï¼š
```python
# å‡è®¾ä½ ä¸‹è½½åˆ°äº† D:\ModelScope_Cache\hub\models\deepseek-ai\DeepSeek-OCR-2
model_path = r"D:\ModelScope_Cache\hub\models\deepseek-ai\DeepSeek-OCR-2"

# åŠ è½½æ—¶ç›´æ¥ä¼ è·¯å¾„
model = AutoModel.from_pretrained(model_path, ...)
```
æ³¨æ„äº‹é¡¹ï¼š

1.**è·¯å¾„åˆ†éš”ç¬¦**: åœ¨ Windows ä¸Šå†™è·¯å¾„å­—ç¬¦ä¸²æ—¶ï¼Œå»ºè®®åœ¨å­—ç¬¦ä¸²å‰åŠ  `r`ï¼ˆå¦‚ `r"D:\Models"`ï¼‰æˆ–è€…ä½¿ç”¨åŒåæ–œæ  `\\`ï¼Œé˜²æ­¢è½¬ä¹‰å­—ç¬¦æŠ¥é”™ã€‚

2.**HuggingFace å…¼å®¹æ€§**: å¦‚æœä½ ä¸æ˜¯ç”¨ modelscope çš„åº“ï¼Œè€Œæ˜¯ç”¨ `transformers` åº“åŠ è½½è¿™ä¸ªä¸‹è½½å¥½çš„æ¨¡å‹ï¼Œä½ éœ€è¦ç¡®ä¿ä¼ å…¥çš„æ˜¯åŒ…å« `config.json` å’Œ `.bin/.safetensors` æ–‡ä»¶çš„æœ€ç»ˆç›®å½•è·¯å¾„ã€‚

å¼ºçƒˆå»ºè®®è®¾ç½®æ°¸ä¹…ç¯å¢ƒå˜é‡ `MODELSCOPE_CACHE` åˆ° D ç›˜ï¼Œè¿™æ ·æ—¢èƒ½èŠ‚çœ C ç›˜ç©ºé—´ï¼Œåˆèƒ½åœ¨åç»­ä½¿ç”¨ä¸­ä¿æŒä»£ç ç®€æ´ï¼ˆä¸éœ€è¦æ¯æ¬¡éƒ½æ‰‹åŠ¨æ”¹è·¯å¾„ï¼‰ã€‚
:::
# æ¨¡å‹ä½¿ç”¨
## æ–‡ä»¶å¤¹åˆ›å»º

åœ¨æºç æ ¹ç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶å¤¹ï¼š`img_input`ã€`img_output`ã€`pdf_input`ã€`pdf_output`ï¼Œåˆ†åˆ«ç”¨äºå­˜æ”¾æ–‡ä»¶

## pdfè½¬md

winç³»ç»Ÿä¸‹ä½¿ç”¨deepseek-ocr2æˆ‘ä»¬æ— æ³•å®‰è£…`vllm`å’Œ`flash-attn`ï¼Œæ•…éœ€è¦é’ˆå¯¹**pdfè½¬md**è¿›è¡Œä¿®æ”¹ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ç±»ä¼¼å›¾ç‰‡è½¬mdçš„åŸºäº`transformers`çš„çº¯åŸç”Ÿæ–¹æ¡ˆ,è™½ç„¶æ¨ç†é€Ÿåº¦æ¯” vLLM ç¨æ…¢ï¼Œä½†èƒœåœ¨Windows å…¼å®¹æ€§å®Œç¾ï¼Œä¸”ä¸éœ€è¦å¤æ‚çš„ç¼–è¯‘ç¯å¢ƒã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨`DeepSeek-OCR2-hf`æ–‡ä»¶å¤¹ä¸­åˆ›å»º`run_dpsk_ocr2_pdf.py`æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š
```python
import os
import io
import fitz  # PyMuPDF
import torch
import sys
import contextlib
import re
from PIL import Image
from transformers import AutoModel, AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================
PDF_PATH = r' '#æ­¤å¤„å¡«å…¥pdf_inputä¸‹çš„pdfè·¯å¾„ï¼Œæ¨èä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œè·¯å¾„å†…å®¹å¿…é¡»ä»¥.pdfç»“å°¾,ä¾‹å¦‚ï¼šä¾‹å¦‚ï¼šDï¼š...\pdf_output\2500836.pdf
MODEL_PATH = r'D:\ModelScope_Cache\models\deepseek-ai\DeepSeek-OCR-2'#æ­¤å¤„å¡«å…¥ä½ çš„æ¨¡å‹æƒé‡æ‰€åœ¨è·¯å¾„
OUTPUT_MD_PATH = r' '#æ­¤å¤„å¡«å…¥pdf_outputä¸‹çš„mdè·¯å¾„ï¼Œæ¨èä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œè·¯å¾„å†…å®¹å¿…é¡»ä»¥.mdç»“å°¾ï¼Œå³åœ¨æ­¤éœ€å¡«å…¥ç›®å½•ä¸‹çš„mdæ–‡ä»¶ï¼Œä½†ä¸å¿…çœŸæœ‰æ­¤æ–‡ä»¶ï¼Œè½¬æ¢åä¼šè‡ªåŠ¨åˆ›å»ºï¼Œä½†æ˜¯æœ€åçš„è·¯å¾„ç»“å°¾ä¸èƒ½æ˜¯æ–‡ä»¶å¤¹ï¼ä¾‹å¦‚ï¼šDï¼š...\pdf_output\A2500836.md
# ===========================================

def clean_ocr_output(text):
    """
    æ¸…æ´—å™¨ï¼šä¸“é—¨å»é™¤æ¨¡å‹è¾“å‡ºä¸­çš„åƒåœ¾æ—¥å¿—
    """
    lines = text.split('\n')
    clean_lines = []
    
    # å®šä¹‰åƒåœ¾æ—¥å¿—çš„ç‰¹å¾å…³é”®è¯
    junk_keywords = [
        "The attention mask", "Setting `pad_token_id`", "The `seen_tokens` attribute",
        "BASE:  torch.Size", "PATCHES:  torch.Size", "UserWarning:", 
        "configuration_utils.py", "modeling_deepseekocr2.py", "warnings.warn",
        "Loading checkpoint", "input_ids", "attention_mask", "position_ids",
        "Using `bitsandbytes`", "was not set", "cannot be inferred",
        "Using the slow distutils backend", "get_max_cache", "transitioning from computing"
    ]

    for line in lines:
        is_junk = False
        # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«åƒåœ¾å…³é”®è¯
        for kw in junk_keywords:
            if kw in line:
                is_junk = True
                break
        
        # 2. æ£€æŸ¥æ˜¯å¦æ˜¯çº¯ç²¹çš„è¿›åº¦æ¡æˆ–åˆ†å‰²çº¿ï¼ˆå¦‚æœä¸æ˜¯Markdownçš„ä¸€éƒ¨åˆ†ï¼‰
        if line.strip().startswith("=====") and "torch.Size" not in line:
             # æœ‰äº›åˆ†å‰²çº¿å¯èƒ½æ˜¯æ—¥å¿—çš„ï¼Œæœ‰äº›æ˜¯æ–‡æ¡£çš„ï¼Œè¿™é‡Œä¿å®ˆå¤„ç†
             pass 

        if not is_junk:
            clean_lines.append(line)
            
    # é‡æ–°ç»„åˆæ–‡æœ¬
    result = '\n'.join(clean_lines)
    
    # 3. å»é™¤é¦–å°¾å¤šä½™ç©ºè¡Œ
    result = result.strip()
    return result

def load_pdf_as_images(pdf_path, dpi=144):
    print(f"[1/3] Loading PDF: {pdf_path}")
    images = []
    try:
        pdf_document = fitz.open(pdf_path)
        zoom = dpi / 72.0
        matrix = fitz.Matrix(zoom, zoom)
        for page_num in range(pdf_document.page_count):
            page = pdf_document[page_num]
            pixmap = page.get_pixmap(matrix=matrix, alpha=False)
            img = Image.open(io.BytesIO(pixmap.tobytes("png"))).convert("RGB")
            images.append(img)
        pdf_document.close()
        return images
    except Exception as e:
        print(f"Error: {e}")
        return []

def main():
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'
    
    print(f"[2/3] Loading Model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    try:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, _attn_implementation='flash_attention_2')
    except:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True)

    model = model.eval().cuda().to(torch.bfloat16)
    
    images = load_pdf_as_images(PDF_PATH)
    if not images: return

    full_markdown_content = ""
    print(f"[3/3] Start Recognition ({len(images)} pages)...")
    
    prompt = "<image>\nFree OCR."

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(OUTPUT_MD_PATH), exist_ok=True)

    for i, img in enumerate(images):
        print(f"Processing Page {i+1}/{len(images)}...", end="")
        
        temp_img_path = f"temp_page_{i}.png"
        img.save(temp_img_path)
        
        captured_output = io.StringIO()
        
        try:
            # æ•è·è¾“å‡º
            with contextlib.redirect_stdout(captured_output):
                # åŒæ—¶ä¹Ÿæ•è· stderr é˜²æ­¢è­¦å‘Šæ¼ç½‘
                with contextlib.redirect_stderr(captured_output):
                    model.infer(
                        tokenizer, 
                        prompt=prompt, 
                        image_file=temp_img_path, 
                        output_path="./temp_output", # è¿™ä¸ªè·¯å¾„ä¸é‡è¦
                        base_size=1024, 
                        image_size=768, 
                        crop_mode=True, 
                        save_results=False
                    )
            
            # è·å–åŸå§‹æ‚ä¹±æ–‡æœ¬
            raw_text = captured_output.getvalue()
            
            # === æ ¸å¿ƒæ­¥éª¤ï¼šæ¸…æ´—æ–‡æœ¬ ===
            clean_text = clean_ocr_output(raw_text)
            
            # æ‹¼æ¥åˆ°ç»“æœ
            if clean_text:
                full_markdown_content += f"\n\n<!-- Page {i+1} -->\n\n" + clean_text
                print(f" Done. (Len: {len(clean_text)})")
            else:
                print(f" Warning: No content.")

        except Exception as e:
            print(f" Error: {e}")
        finally:
            captured_output.close()
            if os.path.exists(temp_img_path):
                try: os.remove(temp_img_path) 
                except: pass

    # æœ€ç»ˆä¿å­˜
    with open(OUTPUT_MD_PATH, "w", encoding="utf-8") as f:
        f.write(full_markdown_content)
    
    print(f"\nSaved clean markdown to: {OUTPUT_MD_PATH}")

if __name__ == "__main__":
    main()
```
ä¿®æ”¹ä¸Šè¿°ä»£ç ä¸­çš„`PDF_PATH`ã€`MODEL_PATH`å’Œ`OUTPUT_MD_PATH`åå¹¶è¿è¡Œå³å¯å¾—åˆ°è½¬æ¢ç»“æœã€‚æ³¨æ„ï¼Œ**`OUTPUT_MD_PATH`å¿…é¡»ä»¥mdä¸ºç»“å°¾ï¼Œä¸èƒ½ä»¥æ–‡ä»¶å¤¹ç»“å°¾ï¼Œä¾‹å¦‚`Dï¼š...\pdf_output\A2500836.md`ï¼Œä½†ä½ ä¸å¿…æå‰åˆ›å»ºæ­¤æ–‡ä»¶**

è‹¥ä½ æƒ³æ‰¹é‡ä¸€æ¬¡æ€§å¤„ç†å®Œ`pdf_input`ä¸‹çš„æ‰€æœ‰pdfå¹¶è½¬ä¸ºå¯¹åº”åç§°çš„mdæ–‡æ¡£ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼š
```python
import os
import io
import fitz  # PyMuPDF
import torch
import sys
import contextlib
import time
from PIL import Image
from transformers import AutoModel, AutoTokenizer

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ =================

# 1. è¾“å…¥æ–‡ä»¶å¤¹ (å­˜æ”¾æ‰€æœ‰ PDF çš„ç›®å½•)
INPUT_DIR = r'D:\..\deepseek-ocr2\DeepSeek-OCR-2\pdf_input'#æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 2. è¾“å‡ºæ–‡ä»¶å¤¹ (å­˜æ”¾ç”Ÿæˆ MD çš„ç›®å½•)
OUTPUT_DIR = r'D:\..\deepseek-ocr2\DeepSeek-OCR-2\pdf_output'#æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 3. æ¨¡å‹è·¯å¾„
MODEL_PATH = r'D:\ModelScope_Cache\models\deepseek-ai\DeepSeek-OCR-2'#æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# ===============================================

def clean_ocr_output(text):
    """ æ¸…æ´—å™¨ï¼šå»é™¤æ¨¡å‹è¾“å‡ºä¸­çš„åƒåœ¾æ—¥å¿— """
    lines = text.split('\n')
    clean_lines = []
    # å®šä¹‰åƒåœ¾æ—¥å¿—ç‰¹å¾
    junk_keywords = [
        "The attention mask", "Setting `pad_token_id`", "The `seen_tokens` attribute",
        "BASE:  torch.Size", "PATCHES:  torch.Size", "UserWarning:", 
        "configuration_utils.py", "modeling_deepseekocr2.py", "warnings.warn",
        "Loading checkpoint", "input_ids", "attention_mask", "position_ids",
        "Using `bitsandbytes`", "was not set", "cannot be inferred",
        "Using the slow distutils backend", "get_max_cache", "transitioning from computing"
    ]

    for line in lines:
        is_junk = False
        for kw in junk_keywords:
            if kw in line:
                is_junk = True
                break
        if not is_junk:
            clean_lines.append(line)
            
    return '\n'.join(clean_lines).strip()

def load_pdf_as_images(pdf_path, dpi=144):
    images = []
    try:
        pdf_document = fitz.open(pdf_path)
        zoom = dpi / 72.0
        matrix = fitz.Matrix(zoom, zoom)
        for page_num in range(pdf_document.page_count):
            page = pdf_document[page_num]
            pixmap = page.get_pixmap(matrix=matrix, alpha=False)
            img = Image.open(io.BytesIO(pixmap.tobytes("png"))).convert("RGB")
            images.append(img)
        pdf_document.close()
        return images
    except Exception as e:
        print(f"Error loading PDF {pdf_path}: {e}")
        return []

def process_one_pdf(pdf_path, model, tokenizer):
    """ å¤„ç†å•ä¸ª PDF æ–‡ä»¶çš„æ ¸å¿ƒé€»è¾‘ """
    
    # 1. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    file_name = os.path.basename(pdf_path) # ä¾‹å¦‚: 2500836.pdf
    file_base_name = os.path.splitext(file_name)[0] # ä¾‹å¦‚: 2500836
    output_md_path = os.path.join(OUTPUT_DIR, f"{file_base_name}.md")
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤è·‘ (å¯é€‰)
    # if os.path.exists(output_md_path):
    #     print(f"   [è·³è¿‡] æ–‡ä»¶å·²å­˜åœ¨: {file_name}")
    #     return

    print(f"\nğŸ“„ æ­£åœ¨å¤„ç†: {file_name} -> {file_base_name}.md")
    
    # 2. è½¬å›¾ç‰‡
    images = load_pdf_as_images(pdf_path)
    if not images:
        print("   [é”™è¯¯] PDFåŠ è½½å¤±è´¥æˆ–ä¸ºç©ºã€‚")
        return

    full_content = f"# {file_base_name}\n\n"
    prompt = "<image>\nFree OCR."
    
    # 3. é€é¡µè¯†åˆ«
    start_time = time.time()
    for i, img in enumerate(images):
        print(f"   - Page {i+1}/{len(images)} ... ", end="", flush=True)
        
        temp_img_path = f"temp_{file_base_name}_{i}.png"
        img.save(temp_img_path)
        
        captured_output = io.StringIO()
        try:
            with contextlib.redirect_stdout(captured_output):
                with contextlib.redirect_stderr(captured_output):
                    model.infer(
                        tokenizer, 
                        prompt=prompt, 
                        image_file=temp_img_path, 
                        output_path="./temp_dummy", 
                        base_size=1024, 
                        image_size=768, 
                        crop_mode=True, 
                        save_results=False
                    )
            
            raw_text = captured_output.getvalue()
            clean_text = clean_ocr_output(raw_text)
            
            if clean_text:
                full_content += f"\n\n<!-- Page {i+1} -->\n\n" + clean_text
                print(f"OK ({len(clean_text)} chars)")
            else:
                print("Warning (Empty)")
                
        except Exception as e:
            print(f"Error: {e}")
        finally:
            captured_output.close()
            if os.path.exists(temp_img_path):
                try: os.remove(temp_img_path)
                except: pass

    # 4. ä¿å­˜ç»“æœ
    try:
        with open(output_md_path, "w", encoding="utf-8") as f:
            f.write(full_content)
        elapsed = time.time() - start_time
        print(f"   âœ… å®Œæˆï¼è€—æ—¶: {elapsed:.1f}s, ä¿å­˜è‡³: {output_md_path}")
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")

def main():
    # 0. åˆå§‹åŒ–
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("ğŸ¤– [1/3] æ­£åœ¨åŠ è½½æ¨¡å‹ (DeepSeek-OCR2)...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    try:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, _attn_implementation='flash_attention_2')
        print("   -> Flash Attention: ON (åŠ é€Ÿæ¨¡å¼)")
    except:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True)
        print("   -> Flash Attention: OFF (æ™®é€šæ¨¡å¼)")

    model = model.eval().cuda().to(torch.bfloat16)

    # 1. æ‰«ææ–‡ä»¶
    print(f"\nğŸ“‚ [2/3] æ‰«æè¾“å…¥ç›®å½•: {INPUT_DIR}")
    pdf_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith('.pdf')]
    
    if not pdf_files:
        print("   æ²¡æœ‰æ‰¾åˆ° PDF æ–‡ä»¶ï¼è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return
    
    print(f"   æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶ï¼Œå‡†å¤‡å¼€å§‹å¤„ç†ã€‚")

    # 2. æ‰¹é‡å¾ªç¯
    print("\nğŸš€ [3/3] å¼€å§‹æ‰¹é‡è½¬æ¢ä»»åŠ¡...")
    for idx, pdf_file in enumerate(pdf_files):
        print(f"\n[{idx+1}/{len(pdf_files)}] ä»»åŠ¡å¯åŠ¨ --------------------------")
        pdf_full_path = os.path.join(INPUT_DIR, pdf_file)
        process_one_pdf(pdf_full_path, model, tokenizer)
        
    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()
```
>**tips:**è¯·ç¡®ä¿ä½ çš„ `INPUT_DIR` è·¯å¾„é‡ŒçœŸçš„æœ‰ PDF æ–‡ä»¶ã€‚

## å›¾ç‰‡è½¬md
æ‰“å¼€`DeepSeek-OCR2-hf`ä¸‹çš„`run_dpsk_ocr2.py`,åœ¨`model_name`ã€`image_file`ã€`output_path`ä¸‹å¡«å…¥ä½ å…·ä½“çš„è·¯å¾„ï¼Œæ³¨æ„ä½¿ç”¨`r`è½¬ä¹‰ï¼Œæ­¤`output_path`ä»¥æ–‡ä»¶å¤¹`img_output`ç»“å°¾å³å¯ã€‚è‹¥ä½ æƒ³æ‰¹é‡å¤„ç†`img_input`ä¸‹çš„å›¾ç‰‡ï¼Œå¯å‚è€ƒä¸‹é¢çš„ä»£ç ï¼Œæä¾›äº†ä¸€å¯¹ä¸€mdå’Œåˆå¹¶mdçš„ç‰ˆæœ¬ï¼š
```python
import os
import io
import torch
import sys
import contextlib
import time
from transformers import AutoModel, AutoTokenizer

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ =================

# 1. è¾“å…¥æ–‡ä»¶å¤¹ (å­˜æ”¾å›¾ç‰‡çš„ç›®å½•)
INPUT_DIR = r'D:\..\deepseek-ocr2\DeepSeek-OCR-2\img_input' #æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 2. è¾“å‡ºæ–‡ä»¶å¤¹ (å­˜æ”¾ç”Ÿæˆ MD çš„ç›®å½•)
OUTPUT_DIR = r'D:\..\deepseek-ocr2\DeepSeek-OCR-2\img_output' #æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 3. æ¨¡å‹è·¯å¾„
MODEL_PATH = r'D:\ModelScope_Cache\models\deepseek-ai\DeepSeek-OCR-2'#æ­¤å¤„ä¸ºç¤ºä¾‹ï¼Œè¯·ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„

# 4. æç¤ºè¯
PROMPT = "<image>\n<|grounding|>Convert the document to markdown."

# ===============================================

def clean_ocr_output(text):
    """ æ¸…æ´—å™¨ï¼šå»é™¤åƒåœ¾æ—¥å¿— """
    lines = text.split('\n')
    clean_lines = []
    junk_keywords = [
        "The attention mask", "Setting `pad_token_id`", "The `seen_tokens` attribute",
        "BASE:  torch.Size", "PATCHES:  torch.Size", "UserWarning:", 
        "configuration_utils.py", "modeling_deepseekocr2.py", "warnings.warn",
        "Loading checkpoint", "input_ids", "attention_mask", "position_ids",
        "get_max_cache", "transitioning from computing"
    ]
    for line in lines:
        if not any(kw in line for kw in junk_keywords):
            clean_lines.append(line)
    return '\n'.join(clean_lines).strip()

def process_batch_images(model, tokenizer):
    """ æ‰¹é‡å¤„ç†å›¾ç‰‡å¹¶åˆ†åˆ«ä¿å­˜
        è‡ªåŠ¨è¯»å– INPUT_DIR ä¸‹çš„ .jpg, .png ç­‰æ ¼å¼çš„å›¾ç‰‡
        å¦‚æœè¾“å…¥æ˜¯ img_01.pngï¼Œå®ƒä¼šè‡ªåŠ¨ä¿å­˜ä¸º img_01.md
    """
    
    # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.webp', '.tiff')
    
    # æ‰«ææ–‡ä»¶
    image_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(valid_extensions)]
    # æŒ‰æ–‡ä»¶åæ’åºï¼Œä¿è¯å¤„ç†é¡ºåº
    image_files.sort() 
    
    if not image_files:
        print(f"âŒ åœ¨ {INPUT_DIR} æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ã€‚")
        return

    print(f"ğŸš€ æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    for idx, img_name in enumerate(image_files):
        img_path = os.path.join(INPUT_DIR, img_name)
        file_base_name = os.path.splitext(img_name)[0]
        output_md_path = os.path.join(OUTPUT_DIR, f"{file_base_name}.md")
        
        print(f"[{idx+1}/{len(image_files)}] å¤„ç†: {img_name} ... ", end="", flush=True)

        captured_output = io.StringIO()
        try:
            # æ ¸å¿ƒï¼šæ‹¦æˆªæ§åˆ¶å°è¾“å‡º
            with contextlib.redirect_stdout(captured_output):
                with contextlib.redirect_stderr(captured_output):
                    model.infer(
                        tokenizer, 
                        prompt=PROMPT, 
                        image_file=img_path, 
                        output_path="./temp_dummy", # è¿™é‡Œçš„è·¯å¾„ä¸é‡è¦
                        base_size=1024, 
                        image_size=768, 
                        crop_mode=True, 
                        save_results=False
                    )
            
            # æ¸…æ´—æ•°æ®
            raw_text = captured_output.getvalue()
            clean_text = clean_ocr_output(raw_text)
            
            # ä¿å­˜å•ä¸ªMD
            if clean_text:
                with open(output_md_path, "w", encoding="utf-8") as f:
                    f.write(clean_text)
                print(f"âœ… å·²ä¿å­˜ -> {file_base_name}.md")
            else:
                print("âš ï¸ å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜ã€‚")
                
        except Exception as e:
            print(f"âŒ å¤±è´¥: {e}")
        finally:
            captured_output.close()

def merge_markdown_files(merged_file_name="merged_result.md"):
    """ 
    åˆå¹¶æ¥å£ï¼šå°† OUTPUT_DIR ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ª 
    å°è¯•æ ¹æ®æ–‡ä»¶åé‡Œçš„æ•°å­—è¿›è¡Œæ’åºã€‚ä¾‹å¦‚ï¼Œå®ƒèƒ½æ­£ç¡®åœ°æŠŠ page_2.md æ’åœ¨ page_10.md å‰é¢ã€‚
    åˆå¹¶åçš„æ–‡ä»¶ä¼šä¿å­˜åœ¨ OUTPUT_DIR ä¸‹ï¼Œåå­—å« all_images_merged.mdï¼ˆä½ å¯ä»¥åœ¨ main å‡½æ•°é‡Œæ”¹åï¼‰ã€‚
    """
    print(f"\nğŸ”— æ­£åœ¨åˆå¹¶æ‰€æœ‰ Markdown æ–‡ä»¶...")
    
    # è·å–æ‰€æœ‰mdæ–‡ä»¶å¹¶æ’åº
    md_files = [f for f in os.listdir(OUTPUT_DIR) if f.lower().endswith('.md')]
    # è¿‡æ»¤æ‰å¯èƒ½å­˜åœ¨çš„åˆå¹¶ç»“æœæ–‡ä»¶æœ¬èº«ï¼Œé˜²æ­¢é€’å½’
    md_files = [f for f in md_files if f != merged_file_name]
    
    # æ™ºèƒ½æ’åºï¼šå°è¯•æŒ‰æ•°å­—é¡ºåºæ’åº (æ¯”å¦‚ 1.md, 2.md, 10.md)ï¼Œè€Œä¸æ˜¯é»˜è®¤çš„å­—ç¬¦é¡ºåº (1.md, 10.md, 2.md)
    try:
        md_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x))) if any(char.isdigit() for char in x) else x)
    except:
        md_files.sort() # å¦‚æœæ–‡ä»¶åä¸å«æ•°å­—ï¼Œå›é€€åˆ°é»˜è®¤æ’åº

    if not md_files:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„ MD æ–‡ä»¶ã€‚")
        return

    merged_path = os.path.join(OUTPUT_DIR, merged_file_name)
    
    with open(merged_path, 'w', encoding='utf-8') as outfile:
        outfile.write(f"# Merged Document\n\nGenerated on {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        for md_file in md_files:
            file_path = os.path.join(OUTPUT_DIR, md_file)
            with open(file_path, 'r', encoding='utf-8') as infile:
                content = infile.read()
                
                # æ·»åŠ æ–‡ä»¶åˆ†å‰²æ ‡è¯†
                outfile.write(f"\n\n--- Source: {md_file} ---\n\n")
                outfile.write(content)
    
    print(f"ğŸ‰ åˆå¹¶å®Œæˆï¼æ€»æ–‡ä»¶å·²ä¿å­˜è‡³:\n{merged_path}")

def main():
    # 0. ç¯å¢ƒè®¾ç½®
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'
    
    # 1. åŠ è½½æ¨¡å‹
    print("ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    try:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, _attn_implementation='flash_attention_2')
    except:
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True)
    
    model = model.eval().cuda().to(torch.bfloat16)

    # 2. æ‰§è¡Œæ‰¹é‡å¤„ç†
    process_batch_images(model, tokenizer)

    # 3. æ‰§è¡Œåˆå¹¶ (å¦‚æœä½ ä¸æƒ³åˆå¹¶ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ)
    merge_markdown_files(merged_file_name="all_images_merged.md")

if __name__ == "__main__":
    main()
```

